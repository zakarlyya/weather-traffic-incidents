{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Table_Cleanup",
      "provenance": [],
      "authorship_tag": "ABX9TyOopsOn9tBBe20BWfdF+4zI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2022/Project-Incident-Team2/blob/master/Table_Cleanup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports and installations\n",
        "!pip install boto3 pandas plotly\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from google.colab import files\n",
        "import boto3, json"
      ],
      "metadata": {
        "id": "kHYvZtMbbXnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the merged table (Incidents, Traffic, Weather, Roads)\n",
        "df = pd.read_csv('merged.csv')"
      ],
      "metadata": {
        "id": "wc1QuVREbyXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping duplicate/unnecessary columns that I renamed as a result of merging\n",
        "df = df.drop(columns=['county', 'month_1', 'hour_of_day_1', 'day_1', 'window_of_day_1', 'day_of_week_1', 'window_id_1'])"
      ],
      "metadata": {
        "id": "XSXVWKcdbXqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# during several transformations, data also transformed types (int -> double), so we need to change it back\n",
        "df['weekend_or_not'] = df['weekend_or_not'].fillna(0).astype(int)\n",
        "df['response_time_sec'] = df['response_time_sec'].fillna(0).astype(int)\n",
        "df['year'] = df['year'].fillna(0).astype(int)\n",
        "df['month'] = df['month'].fillna(0).astype(int)\n",
        "df['xdgroup'] = df['xdgroup'].fillna(0).astype(int)\n",
        "df['hour_of_day'] = df['hour_of_day'].fillna(0).astype(int)\n",
        "df['day'] = df['day'].fillna(0).astype(int)\n",
        "df['window_of_day'] = df['window_of_day'].fillna(0).astype(int)\n",
        "df['incident_id'] = df['incident_id'].fillna(0).astype(int)\n",
        "df['segid'] = df['segid'].fillna(0).astype(int)\n",
        "df['xd_id'] = df['xd_id'].fillna(0).astype(int)\n",
        "df['xdsegid'] = df['xdsegid'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "oNjZ-hyUbXve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = pa.Table.from_pandas(df)\n",
        "pq.write_table(table, 'everything_merged.parquet')"
      ],
      "metadata": {
        "id": "J9J86uTab1Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can locally download the results here\n",
        "files.download('everything_merged.parquet')"
      ],
      "metadata": {
        "id": "SXRvqOZpcB3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or alternatively, can link to AWS to write to an S3 Bucket\n",
        "credentials = {\n",
        "    'region_name': AWS_REGION,\n",
        "    'aws_access_key_id': AWS_ACCESS_KEY,\n",
        "    'aws_secret_access_key': AWS_SECRET_KEY,\n",
        "    'aws_session_token': AWS_SESSION_TOKEN\n",
        "}\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')\n",
        "s3_url = 's3://bigdata-seg/outputs/everything_merged.parquet'\n",
        "df.to_parquet(s3_url)"
      ],
      "metadata": {
        "id": "K-27snDbcfdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
